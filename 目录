第 1 章 Bandit 问题
    1.1 机器学习分类和强化学习
        1.1.1 监督学习
        1.1.2 无监督
        学习 1.1.3 强化学习
    1.2 Bandit 问题
        1.2.1 什么是 Bandit 问题
        1.2.2 什么是好的老虎机
        1.2. 3用数学公式表示
    1.3 Bandit 算法
        1.3.1 价值估计方法
        1.3.2 计算平均值的实现
        1.3.3 玩家策略
    1.4 Bandit 算法实现
        1.4.1 老虎机实现
        1.4.2 Agent 实现
        1.4. 3 尝试移动
        1.4.4算法的平均性质
    1.5 非平稳问题
        1.5.1 求解
        非平稳问题 1.5.2 求解非平稳问题
    1.6 小结

第二章马尔可夫决策过程
    2.1 什么是MDP？
        2.1.1 MDP的具体例子
        2.1.2 代理与环境的交互
    2.2 环境与代理的制定
        2.2.1 状态转换
        2.2.2 奖励函数
        2.2.3 代理策略
    2.3 MDP目标
        2.3.1 情节任务和连续任务
        2.3.2 利润
        2.3.3 状态值函数
        2.3.4 最优策略和最优值函数
    2.4 MDP 示例
        2.4.1 备份图
        2.4.2 寻找最优策略
    2.5 总结

第三章贝尔曼方程
    3.1 贝尔曼方程的推导
        3.1.1 概率和期望值 ( 3.1.2贝尔曼
        方程的
    推导 3.2 贝尔曼方程的例子
        3.2.1 2 方格世界
        3.2.2 贝尔曼方程的意义
    3.3 作用值函数和贝尔曼方程
        3.3.1 作用值函数
        3.3.2 使用作用值函数的
    贝尔曼方程 3.4 贝尔曼最优方程
        3.4.1 状态值函数中的
        贝尔曼最优方程 3.4.2 Q 函数
    中的贝尔曼最优方程 3.5 贝尔曼最优方程的例子
        3.5.1 贝尔曼最优方程的应用
        3.5. 2得到优化措施
    3.6 总结

第 4 章动态规划
    4.1 动态规划与策略评估
        4.1.1 动态规划概述
        4.1.2 尝试
        迭代策略评估 4.1.3 迭代策略评估的另一种实现
    4.2 解决更大的问题
        4.2.1 GridWorld类的实现
        4.2 .2 如何使用defaultdict 
        4.2.3 迭代方法评估的实现
    4.3 策略迭代方法
        4.3.1 策略的改进
        4.3.2 重复评估和改进
    4.4 策略迭代方法的实现
        4.4 .1 度量改进
        4.4.2 重复评估和改进
    4.5 价值迭代法
        4.5.1 值迭代法的推导
        4.5.2 值迭代法的实现
    4.6 小结

第5章蒙特卡洛方法
    5.1 蒙特卡洛方法的基础
        5.1.1 骰子
        和 5.1.2 分布模型和样本模型
        5.1.3 蒙特卡洛的实现方法
    5.2 蒙特卡洛方法评估措施
        5.2.1 蒙特卡洛方法求值函数
        5.2.2 求所有状态的值函数
        5.2.3 蒙特卡洛方法的有效实现
    5.3 蒙特卡洛方法的实现
        5.3 .1 阶梯法
        5.3.2 代理类的实现
        5.3.3 工作蒙特卡洛方法
    5.4 蒙特卡洛方法测量控制
        5.4.1 评估和改进
        5.4.2 使用蒙特卡洛方法实现策略控制
        5.4.3 ε-贪心法（第一次修改）
        5.4 .4 到定值α法（第二次修改）
        5.4.5 [修改版本] 使用蒙特卡洛方法的策略迭代方法的实现
    5.5 测量偏离类型和重要性抽样
        5.5.1 测量类型和偏离类型
        5.5.2 重要性抽样
        5.5.3 如何减少方差
    5.6 小结

第 6 章 TD 方法
    6.1 TD 方法的测量评估
        6.1.1 TD 方法的推导
        6.1 .2 MC 方法与 TD 方法的比较
        6.1.3 TD 方法的实现
    6.2 SARSA 
        6.2.1 Measure-on 类型 SARSA 
        6.2.2 SARSA 的实现
    6.3 Measure-off 类型 SARSA 
        6.3.1 Measure-off 类型和重要性抽样
        6.3 .2 Policy Off-type SARSA 实现
    6.4 Q-learning 
        6.4.1 Bellman 方程和 SARSA 
        6.4.2 Bellman 最优方程和 Q-learning 
        6.4.3 Q-learning 实现
    6.5 分布模型和样本模型
        6.5.1 分布模型和样本模型
        6.5.2 样本模型的Q-learning 
    6.6 总结

第 7 章神经网络和 Q 学习
    7.1 DeZero 的基础知识
        7.1.1 使用 DeZero 
        7.1.2 多维数组（Tensol）和函数
        7.1.3 优化
    7.2 线性回归
        7.2.1 玩具数据集
        7.2.2 线性回归理论
        7.2.3 实现线性回归
    7.3 神经网络
        7.3.1 非线性数据集
        7.3.2 线性变换和激活函数
        7.3.3 神经网络的实现
        7.3.4 层和模型
        7.3.5 优化器
    7.4 Q 学习和神经网络
        7.4.1 神经网络预处理
        7.4 .2 表示Q函数的
        神经网络 7.4.3 神经网络与Q学习
    7.5 总结

第8章DQN 
    8.1 OpenAI Gym 
        8.1.1 OpenAI Gym基础知识
        8.1.2 随机代理
    8.2 DQN 核心技术
        8.2.1 体验重放
        8.2.2 体验重放的实现
        8.2.3 目标网络
        8.2.4 目标网络的实现
        8.2.5 运行DQN 
    8.3 DQN和Atari 
        8.3.1 Atari游戏环境
        8.3. 2 预处理
        8.3.3 CNN 
        8.3.4 其他思路
    8.4 DQN扩展
        8.4.1 双DQN 
        8.4.2 优先体验回放
        8.4.3 决斗DQN 
    8.5 总结

9章Policy Gradient方法
    9.1
        最简单的Policy Gradient方法 9.1.1 
        Policy的推导梯度法 9.1.2 策略梯度法的
        算法 9.1.3 策略梯度法的实现
    9.2 REINFORCE 
        9.2.1 REINFORCE 算法
        9.2.2 实施 REINFORCE 
    9.3 基线
        9.3.1 基线思想
        9.3.2 基线测量梯度方法
    9.4 Actor-Critic 
        9.4.1 Actor-Critic 推导
        9.4.2 Actor-Critic 实施
    9.5 基于测量方法的优点
    9.6 总结

第 10 章进一步
    10.1 深度强化学习算法的分类
    10.2 度量梯度法 系列开发算法
        10.2.1 A3C、A2C 
        10.2.2 DDPG 
        10.2.3 TRPO、PPO 
    10.3 DQN 系列开发算法
        10.3.1 Categorical DQN 
        10.3.2 Noisy Network 
        10.3.3 Rainbow 
        10.3 .4 彩虹之后的开发算法
    10.4 案例研究
        10.4.1 棋盘游戏
        10.4.2 机器人控制
        10.4.3 NAS（神经架构搜索）
        10.4.4 其他示例
    10.5 深度强化学习的挑战和可能性
        10.5.1 在实际系统中的应用
        10.5.2 制定为 MDP
        的技巧 10.5.3 通用人工智能系统
    10.6 总结

附录 A测量 Off-Type Monte Carlo 方法
    A.1 测量 Off-Type Monte Carlo 方法理论
    A.2 测量 Off-Type Monte Carlo 方法实现

附录 B n-Step TD 方法

附录 C 双 DQN 理解
    C.1 高估 什么是
    C.2高估解决方案

附录 D 政策梯度证明
    方法 D.1 政策梯度方法
    推导 D.2 基线

结论
参考
索引的推导
